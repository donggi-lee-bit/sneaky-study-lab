# 5. 레디스 운용 관리

## 5.1 데이터 영속성

#### RDBMS vs 레디스

- RDBMS
  - 버퍼풀(메모리)
  - 기본적으로 디스크
- 레디스
  - 메모리

## 5.1 레디스의 데이터 영속성

레디스에서 영속성 관련 설정을 할 땐 성능과 내구성의 타협점을 찾아 그에 맞게 설정해야함  

레디스는 스냅샷, AOF(Append-Only File) 두 가지 방식으로 데이터의 영속성을 보장하고,  
두 가지 방법을 조합해 스냅샷/AOF/스냅샷+AOF/영속성X 총 4가지 방식으로 영속성 전략을 짤 수 있음  

스냅샷은 특정 시점의 데이터베이스 내용을 RDB(Redis Database) 형식의 파일로 저장하며  
그 파일을 복원하는 형태로 데이터를 재사용할 수 있음  
- 예를 들어 한 시간, 하루, 한 달로 기간을 설정하여 그 시점의 스냅샷을 생성

AOF는 레디스에 쓰기 작업을 수행하면 파일에 데이터를 차례로 기록하는 형식으로 진행  
이 파일을 기반으로 레디스를 시작할 때 로그 파일 데이터를 재생하는 방식으로 데이터를 복원함  

### 스냅샷

특정 시점에 데이터베이스에 있는 내용(특정 시점의 상태)을 RDB에 저장하는 방식  
스냅샷은 자동 혹은 수동으로 생성할 수 있음
- SAVE 명령어 : 동기 방식
- BGSAVE 명령어 : 비동기 방식

레디스 서버 문제 발생 시 마지막 스냅샷 이후의 데이터는 손실될 가능성이 있음  

레디스는 기본적으로 싱글 스레드로 요청을 처리하기 때문에 SAVE 명령어 실행 시  
동일한 스레드 내에 RDB 파일을 생성함  
- 이때 다른 요청을 차단하기 때문에 실제 운영환경에서는 사용되지 않음

BGSAVE는 백그라운드 실행을 통해 RDB 파일을 생성하고, 상세한 프로세스는 다음과 같음  
1. 요청을 처리 중인 프로세스에서 RDB 파일을 덤프하기 위한 자식 프로세스를 fork 처리하여 생성
2. 자식 프로세스는 데이터셋 전체를 임시 RDB 파일에 덤프
3. 덤프 처리가 완료되면 설정된 RDB 파일 이름으로 변경

백그라운드에서 스냅샷 저장 처리에 실패하게 되면, 데이터 손실 가능성이 있으므로  
영속성을 위해 문제를 관리자에게 알리기 위해 쓰기 작업 요청을 처리하지 않음  

### AOF

실시간 백업과 같이 작성 중인 파일 끝에 계속 데이터를 추가하여 데이터의 영속성을 보장하는 방식  

이러한 특성으로 인해 AOF 파일은 스냅샷으로 생성한 RDB 파일보다 크기가 커지는 경향이 있음  

또한 레디스 서버 재시작 시 AOF 파일을 다시 메모리에 적재해야하므로 파일 크기가 크면 클수록 재시작 시간이 길어짐  

#### AOF 자동 생성 시점

AOF는 fsync로 디스크에 쓰기 작업을 수행하며 appendfsync 설정을 통해 자동 생성 시점을 설정할 수 있음
- always
- everysec (기본값)
- no

always의 경우, 쓰기 작업마다 데이터를 버퍼에서 디스크로 플러시하게 됨  
매 작업마다 플러시하기 때문에 문제가 발생하더라도 최대 하나의 명령어만 손실   
이 방식은 데이터 손실에 대한 내구성은 높지만, 성능에 영향을 줄 수 있음    

everysec의 경우, pthread에서 백그라운드 스레드를 생성하여 매초마다 버퍼에서 디스크로 플러시하게 됨  
매초마다 수행되기 때문에 문제가 발생하더라도 최대 1초 동안 실행된 명령어만 손실됨  

no의 경우, 운영체제가 설정한 시점에 실행하게 됨  

#### AOF 메커니즘

AOF 메커니즘은 레디스 7.0 이후로 메커니즘에 변화가 생김  

레디스 7.0 이전 **rewrite 방식**을 통해 AOF 메커니즘이 동작함  

AOF rewrite 방식의 처리 흐름은 다음과 같음
1. 자식 프로세스 생성
- 레디스는 AOF를 다시 쓰기 위해서 현재 메모리 상태를 복사한 자식 프로세스를 만듦  
- 이때 메인 프로세스는 계속 클라이언트의 요청을 처리함  
2. 자식 프로세스에서 새로운 AOF 파일 생성  
- 자식 프로세스는 레디스의 현재 메모리 상태만을 기준으로 새로운 AOF 파일을 작성
3. 작성 완료 후 신호 전송
- 자식 프로세스가 새로운 AOF 파일을 다 만들면, 부모 프로세스에 완료 신호를 보냄
4. 누락된 명령 반영
- 자식 프로세스가 복사된 이후에 클라이언트가 보낸 새로운 명령들은 메인 프로세스가 AOF rewrite 버퍼에 저장해두었다가,  
  이 내용을 자식 프로세스에게 보내고,  
  자식 프로세스는 해당 명령을 새 AOF 파일 끝에 이어 붙임  
5. 파일 교체
- 모든 작업이 끝나면, 기존 AOF 파일을 새로 만든 파일로 원자적으로 교체함

기존 AOF rewrite 과정에서는 많은 메모리 소비와 디스크 I/O가 증가하는 문제가 있었고, 이 문제를 해결하기 위해 multi-part AOF 기능이 도입됨  

multi-part AOF는 AOF 파일을 여러 파트(base + incr)로 나눠서 관리하여,  
기존 AOF의 단일 파일 구조와 rewrite 부하 문제를 해결하는 구조  

multi-part AOF에서의 AOF rewrite 처리 과정은 다음과 같음
1. 자식 프로세스를 fork하여 AOF rewrite 시작
- 클라이언트 요청을 계속 처리하면서, AOF를 재작성하기 위해 자식 프로세스를 fork 함
- (기존 AOF 메커니즘과 동일한 fork 기반 Copy-On-Write 방식)
2. 부모 프로세스에서 새로운 incr AOF 파일 생성 시작
- 자식 프로세스에서 새로운 base 파일을 만드는 동안, 부모 프로세스는 클라이언트의 새로운 쓰기 요청을
  실시간 로그 파일에 기록하고 있음
3. 자식 프로세스가 현재 메모리 상태를 기반으로 새로운 base AOF 파일 생성
- 자식 프로세스는 레디스의 현재 메모리 상태를 그대로 읽어서,
  불필요한 중복 없이 최소한의 명령어만 포함한 AOF 파일을 새롭게 만듦
- 이 파일을 appendonly.aof.base라는 이름으로 저장되며,
  기존에 누적되던 로그보다 훨씬 정돈된 구조를 가짐
- (현 시점의 레디스 상태를 가장 간결하게 다시 만들 수 있도록 구성하는 목적)
4. 부모 프로세스는 클라이언트의 새로운 쓰기 요청을 기존 incr AOF 파일에 계속 기록
- 자식 프로세스가 base AOF 파일을 생성하는 동안에도, 클라이언트의 요청을 계속 처리함
- 그 과정에서 발생하는 새로운 쓰기 명령어들은 기존과 동일하게 appendonly.aof.incr 파일에 실시간으로 순차적으로 기록됨
- 이 incr 파일은 base 파일이 만들어지는 시점 이후의 변경사항을 담고 있으며, 추후 base와 함께 레디스의 복구 기준으로 사용됨
5. 새로운 base와 incr 조합을 기준으로 매니페스트 파일을 갱신
- 자식 프로세스가 base 파일 생성을 마치면, 레디스는 지금 사용 중인 base 파일과 incr 파일의 조합을 기록한 임시 매니페스트 파일을 만들고
- 이 파일은 appendonly.aof.manifest.tmp라는 이름으로 생성되며, 앞으로 레디스가 어떤 AOF 파일들을 기준으로 복구할지를 정의함
- 준비가 끝나면 기존 매니페스트를 이 임시 파일로 원자적으로 교체하여, 새로운 AOF 조합을 공식적인 복구 기준으로 적용함
6. 이전에 사용되던 base/incr 파일은 히스토리 파일로 이동 후 삭제 대상이 됨
- 새로운 base와 incr 조합이 적용되면, 이전에 사용되던 AOF 파일들은 더 이상 복구에 사용되지 않음
- 레디스는 이 파일들을 바로 삭제하지 않고, 우선 appendonly.aof.HIST.<UUID> 형식의 히스토리 파일로 이동시킴
- 이후 내부적으로 정해진 조건(예: 시간이 지나거나 디스크 압박 등)에 따라 자동으로 삭제되어 리소스를 정리하게 되고,
- 이 과정을 통해 레디스는 운영 중인 파일과 백업 파일을 구분하여, 안정성을 유지한채 불필요한 파일도 점진적으로 정리함

### 스냅샷 vs AOF

스냅샷과 AOF의 장단점은 다음과 같음

#### 스냅샷

- 장점
  - 특정 시점의 상태를 덤프하여 파일 크기가 작음
  - 자식 프로세스의 포크 처리 시에만 성능에 영향을 미치며, 나머지 처리는 백그라운드에서 수행되어 성능에 미치는 영향이 작음
  - AOF보다 빠르게 시작할 수 있음
  - 레디스 5.0 이상에서는 LFU 또는 LRU 정보를 포함하여 복원 직후부터 더 정확한 데이터를 관리할 수 있음
- 단점
  - 문제가 발생하면 이전 스냅샷 이후의 데이터가 손실되므로 데이터 손실에 대한 내구성이 AOF에 비해 낮음

#### AOF

- 장점
  - 실시간으로 기록되어 데이터의 내구성이 높음
  - 장애 발생 직전까지의 상태 복원이 가능함
  - 레디스 재시작 시 정확한 데이터 복원이 가능함
- 단점
  - 모든 쓰기 명령을 REDO 로그 형태로 기록하므로 파일 크기가 큼
  - fsync 전략에 따라 쓰기 성능 저하 발생 가능
  - 레디스 재시작 시 복원 시간이 길어짐
  - 일부 명령어(BRPOPLPUSH)의 복원 시 정확히 복원되지 않는 버그가 있음
  - 레디스 7.0 미만에서는 AOF 재작성 시 중복 기록, 메모리 소비 증가, 쓰기 작업 중단 위험 존재

스냅샷과 AOF의 사용 전략은 기본적으로
- 스냅샷 + AOF 혼합 전략을 사용
- AOF가 활성화되어 있으면 AOF가 복구 우선순위를 가짐
- aof-use-rdb-preamble yes 설정을 통해 스냅샷/AOF 혼합 형식 활성화 가능

혼합 형식을 사용하게 되면 스냅샷으로 빠른 복구가 가능하며 동시에 AOF로 정확한 복원이 가능함  

여기서 사용자는 얼마나 자주 덤프할 것인가, 얼만큼 데이터의 손실을 허용할 것인지 서비스의 특성에 맞게  
AOF 빈도, 스냅샷 주기, 혼합 여부를 조정할 필요가 있음  

|판단 항목|설명|
|---|---|
|데이터 손실 허용 여부|조금이라도 손실되지 않아야 할 경우 -> AOF 필수|
|성능 우선 or 내구성 우선|성능: 스냅샷 / 내구성: AOF|
|재시작 복구 속도 중요 여부|중요: 스냅샷|
|파일 크기와 I/O 비용|제한 있다면 스냅샷 위주, AOF 최소화|
|서비스 유형|캐시 서버: 스냅샷 DB 역할: AOF|

### 데이터 삭제 패턴

레디스의 데이터 영속 기능이 있지만, 데이터가 메모리에만 저장된 상태에서는 데이터가 삭제될 가능성이 있음  

삭제된 데이터가 장애 때문인지 혹은 의도하지 않은 조작 때문인지 원인을 파악하고 있어야 함  

#### 데이터가 손실될 수 있는 상황

- 레디스 인스턴스 재시작
- 레디스 서버 장애
- 명령어 실행
  - DEL/HDEL/XDEL
  - FLUSHALL/FLUSHDB
  - UNLINK
- TTL 만료
  - EXPIRE/EXPIREAT/PEXPIRE/PEXPIREA
  - SET 명령어의 EX 옵션
- 강제 제거
- 비동기 레플리케이션
- 레디스 클러스터 간의 네트워크 단절
- 기타
  - 키 이름 재설정(RENAME)
  - 잘못된 데이터베이스 선택
  - 애초에 데이터가 삽입되지 않은 경우

##### 데이터 손실이 발생하는 주요 상황과 대응 전략

- 캐시 노드 및 스토리지 고장과 같은 장애
- 유지보수 등 인위적 조작 또는 시스템에 의한 기계적 처리
- 운영 실수 등 의도치 않은 인위적 조작
- 애플리케이션 버그

캐시 노드나 스토리지 고장과 같은 장애의 경우, 장애가 발생한 동안에는 백엔드 RDBMS에서 원본 데이터를 가져오는 방식으로 구현할 수 있음  
RDBMS에서 증가하는 부하로 인한 영향도 따로 고려할 필요는 없지만(?), 지연 시간의 증가 정도에 그침  

관리형 서비스를 사용하는 경우, 캐시 노드에서 장애가 발생하면 자동으로 노드가 교체되므로 재시도 처리 등 대처를 할 수 있음  
하지만 영속성 기능을 사용하는 경우 충분하지 않을 수 있음. 이 경우엔 레디스를 꼭 사용해야 하는지 검토가 필요함  

중요한 데이터를 일부 레디스 서버에 올려둘 경우, 레디스 레이어만으로는 보장되지 않는 부분을 다른 방법으로 커버할 수 있도록 사전에 대처 방안을 마련해야함  
가령, RDB 파일의 백업을 복사해두고 복원할 수 있다면 캐시 노드나 하드웨어 레벨에서 문제가 발생해도 데이터를 복구할 수 있음  

운영 실수에 의한 데이터 손실을 방지하기 위해 필요 이상의 권한을 부여하지 않도록 하는 방법도 있음  

## 5.2 캐시 서버로서 레디스 아키텍처

레디스 용도는 크게 두 가지로 나뉨  
- 데이터가 사라져도 상관없는 캐시 서버
- 데이터를 영구적으로 저장하는 데이터 저장소

레디스를 다루는 데 있어 지연 로딩(Lazy Loading/Cache-Aside) 패턴과 Write-Through 패턴을 숙지하고 있어야 함

### 5.2.1 읽기 관점 아키텍처

읽기 관점 아키텍처에는 두 가지 패턴이 있음
- 지연 로딩(Lazy Loading/Cache-Aside) 패턴
- Read-Through 패턴

지연 로딩 패턴은 데이터를 읽어오는 관점에서 접근한 아키텍처로,  
원본 데이터는 RDBMS에 저장하고, 레디스는 그 앞단에 배치하는 형태로 사용  

애플리케이션은 다음과 같은 흐름으로 처리
- 애플리케이션은 레디스에 데이터를 요청
- 요청된 데이터가 레디스 서버에 존재하는지, 유효 기간 내인지에 따라 처리
  - 요청된 데이터가 레디스 서버에 존재하고, 유효 기간 내에 있는 경우라면 레디스에서 애플리케이션으로 응답
  - 요청된 데이터가 레디스 서버에 존재하지 않거나, 존재하더라도 유효 기간이 지난 경우라면 다음과 같이 응답
    - 애플리케이션은 백엔드 데이터베이스에서 데이터를 가져옴
    - 애플리케이션은 가져온 데이터를 레디스의 캐시에 업데이트함

이 아키텍처의 주요 장단점은 다음과 같음
- 장점
  - 레디스 캐시 노드 장애 시 다운타임을 줄일 수 있으며,
    백엔드로의 데이터 접근 지연 시간의 영향을 최소화할 수 있음
  - 요청된 데이터만 캐시에 저장하므로, 사용하지 않는 데이터를 캐시로 사용하는 공간이 작음
- 단점
  - 가져온 데이터가 오래되었을 수 있음
  - 캐시 미스 시 오버헤드가 큼

#### Read-Through 패턴

Read-Through 패턴은 지연 읽기 패턴의 변형된 아키텍처인데,  
지연 읽기 패턴과 같이 레디스에 데이터를 요청하고 캐시 미스가 발생했을 때 데이터베이스에서 캐시로 데이터를 읽어오는 방식을 사용함  

지연 읽기 패턴과 가장 큰 차이는 데이터베이스에서 데이터를 읽어오는 작업을 애플리케이션에서 직접 처리할 필요가 없고,  
라이브러리 등을 사용해 데이터베이스에서 레디스로 데이터를 읽어오는 과정을 처리해야 함

### 5.2.2 쓰기 관점 아키텍처

쓰기 관점 아키텍처로는 다음 세 가지 패턴이 있음
- Write-Through 패턴
- Write-Back 패턴
- Write-Around 패턴

#### Write-Through 패턴

Write-Through 패턴은 데이터 쓰기 작업을 할 때의 관점에서 접근한 아키텍처인데,  
이 패턴에서는 다음과 같은 흐름으로 데이터를 처리함  

1. 애플리케이션은 데이터베이스에 데이터를 저장
2. 애플리케이션은 1번과 같은 데이터를 레디스 서버에도 저장

이 아키텍처의 주요 장단점
- 장점
  - 레디스 서버 내의 캐시 데이터가 항상 최신 상태를 유지
  - 읽기 작업 시 오버헤드가 적다
- 단점
  - 사용하지 않는 캐시 데이터 생성 가능성이 있음
  - 쓰기 작업 시 데이터베이스와 캐시에 모두 쓰기를 해야 하므로 오버헤드가 큼

이 패턴은 데이터의 읽기보다 쓰기 작업 때 지연 시간이 다소 증가해도 괜찮은 경우에 효과적이며,  
데이터베이스에 변경이 발생할 때마다 레디스 서버에 쓰기 작업을 수행하므로 사용하지 않는 데이터가 레디스에 저장될 수 있음  

#### Write-Back 패턴

Write-Back 패턴은 캐시에 저장 후, 일정 시간이 지연되면 데이터베이스를 비동기 방식으로 주기적인 업데이트를 하는 방식  
데이터베이스가 RDBMS인 경우, 데이터를 영구적으로 저장하는 시점에 정규화하여 테이블에 저장함  

#### Write-Around 패턴

TBD

### 5.2.3 아키텍처 안티 패턴

지연 로딩 아키텍처에서 캐시 노드가 다운된 경우에 미치는 영향도 사전에 고려해야 함  

백엔드 데이터베이스에서 데이터를 가져옴으로써 애플리케이션의 다운타임을 최소화하고 지연 시간 만으로 문제를 해결할 수 있는지 확인이 필요함  

레디스의 부하로 인해 데이터베이스의 문제가 발생하거나 데이터베이스 클라이언트 측의 설정 등으로 인한 타임아웃에도 주의해야 함

### 5.2.4 데이터 저장소로서의 레디스 아키텍처

레디스를 데이터 저장소로 사용할 경우 주의해야하며, 사용하기 전에 이러한 문제를 해결할 수 있는지 사전에 고려되어야 함  

주의사항은 다음과 같음  
- 사용하는 레디스의 기능이 데이터 저장소로서의 요구사항을 충족하는지 확인
- 문제 발생 시 복구 방법 확인
- 다른 RDB와 결합하여 사용할 경우 롤백 발생 시
  데이터의 일관성 측면에서 애플리케이션에 문제가 없는지 여부를 고려
- 레디스의 레플리케이션 기능이나 스냅샷 생성을 사전에 준비

레디스를 캐시 서버와 데이터 저장소로서 역할이 다름에도 하나의 캐시 노드로 해결하려고 한다면 관리가 복잡해짐  

비용 측면도 고려가 필요하겠지만, 리소스를 추가하고 목적별로 캐시 노드를 분리하는 것이 적절함  

레디스를 유일한 데이터 저장소로 사용할 경우 적절한 유스케이스인지 확인이 필요함  
영속성과 같은 트레이드오프를 고려해야하며, 레디스의 데이터 유형이나 기능을 반드시 사용해야 하는 상황을 제외하고는  
RDBMS가 적합한 경우가 많음  

## 5.3 모범 사례

### 5.3.1 TTL 설정

레디스를 캐시 서버로 사용할 경우 기본적으로 데이터가 소실되어도 문제가 없는 데이터를 저장함  

캐시에 저장하는 데이터 키에는 캐시에 유지될 최소 시간을 TTL로 설정하고, 시간이 지나면 만료되게끔 설정  

이때 TTL을 너무 짧게 설정하면 RDBMS 등 데이터베이스에 부담을 줄 수 있으므로 TTL 값은 적절하게 설정해야함  

적절한 TTL 설정을 통해 데이터를 최신 상태로 유지할 수 있고, 불필요한 데이터로 인한 메모리 부족도 방지할 수 있음  

TTL 설정 시 avg_ttl 값으로 데이터베이스 내의 평균 TTL을 밀리초 단위로 확인할 수 있어 값이 너무 크게 설정되어 있지 않은지 참고할 수 있음  

avg_ttl로 확인한 평균 TTL이 요구사항에 맞는 값이 설정되어 있는지 참고할 수 있는데,  
캐시 서버의 경우 쓰기 작업보다 읽기 작업이 중심일 때 효과적이기 때문에 애플리케이션의 쓰기 작업의 비중이 높다면  
레디스를 캐시 서버로 사용하는 것이 효과가 있는지 고려해보는 게 좋음  

### 5.3.2 제거 정책 설정

레디스의 메모리 사용량이 maxmemory 로 설정된 값에 도달하면, 레디스는 메모리 확보를 위해 키를 eviction하게 됨  

이때 제거 방식은 maxmemory-policy 로 설정된 정책에 따라 결정됨  

레디스를 캐시 서버로 사용하든 데이터 저장소로 사용하든 이 정책을 설정하는 것을 권장하며,  
캐시 서버로 사용하는 경우 다음과 같은 정책 중에서 선택하는 것을 고려해볼 수 있음
- TTL이 설정된 키를 대상으로 하는 volatile-로 시작하는 정책을 선택
- 데이터 저장소로도 사용하는 경우, 모든 키를 대상으로 하는 allkeys-로 시작하는 정책을 선택

LRU, LFU, 랜덤, TTL을 기반으로 한 정책 선택은 애플리케이션과 데이터의 특성을 고려해서 설정해야함  

그러나 메모리 확보를 위한 키 eviction 처리는 TTL을 통한 만료 처리 방식에 비해 처리 비용이 클 수 있어,  
키에 TTL을 설정할 수 있는지를 먼저 확인하는 것이 좋음

### 5.3.3 백업

TBD

### 5.3.4 커넥션 풀링

데이터베이스에 매번 새롭게 연결을 할 경우 비용과 오버헤드가 발생함  

따라서 레디스 클라이언트에서 커넥션 풀링 기능을 제공하는지 확인하고, 커넥션 풀링 기능을 사용하여 처리 효율성을 높일 수 있음  

### 5.3.5 재시도 처리

애플리케이션의 클라이언트와 레디스 서버 간 통신이 일시적으로 중단되었을 때,  
클라이언트 측에서 재시도 처리를 수행하는 게 효과적일 수 있음  

재시도 처리를 수행할 땐 재시도 횟수, 타임아웃 값, 재시도 간격, 재시도 횟수의 상한선 설정 등을 고려해야 함  

저자가 추천하는 재시도 방법은 지수 백오프(Exponential Backoff) 알고리즘에 기반한 방법  
- 처음 접속 시도에 실패하면 짧은 시간 동안 대기한 후 다시 시도
- 실패할 경우 이전 대기 시간의 두 배를 기다린 후 다시 시도
- 이런 식으로 재시도 간격 시간을 지수적으로 늘려가는 방식

### 5.3.6 기타 사례

[https://aws.amazon.com/blogs/database/best-practices-valkey-redis-oss-clients-and-amazon-elasticache/](https://aws.amazon.com/blogs/database/best-practices-valkey-redis-oss-clients-and-amazon-elasticache/)
[https://aws.amazon.com/ko/blogs/database/optimize-redis-client-performance-for-amazon-elasticache/](https://aws.amazon.com/ko/blogs/database/optimize-redis-client-performance-for-amazon-elasticache/)
[https://redis.io/blog/7-redis-worst-practices/](https://redis.io/blog/7-redis-worst-practices/)

## 5.4 캐시 노드 크기 조정

### 5.4.1 크기 조정 기준

캐시 노드 크기를 결정하는 대략적인 기준은 다음과 같음

#### 레디스 클러스터

레디스 클러스터를 구축하는 경우, 최소 세 개의 캐시 노드를 마스터로 설정해야함  
==> quorum(다수결 기준, 분산 환경에서 노드의 장애를 판단하고, 노드를 복구하거나 리더를 선출할지 결정할 때)

단일 마스터 구성으로도 시작은 가능하나, 테스트용 환경일뿐 이 경우 장애 복구가 불가능함  
실제 운영 환경에서는 최소 3개의 마스터 노드가 필수적임

레디스 클러스터를 사용하고자 할 땐 다음을 고려해야함
1. 레디스 클러스터가 필요한지 혹은 일반 레디스 서버만으로도 충분한지
2. 사용 중인 클라이언트가 레디스 클러스터를 지원하는지

레디스 클러스터는 주로 부하가 높은 쓰기 작업을 처리하는 경우나  
높은 가용성이 요구되는 경우에 필요함  

또한 요구되는 메모리 용량이 매우 큰 경우 캐시 노드를 늘려 메모리 용량을 확장할 수 있는 레디스 클러스터가 필요함  

#### 실제 용도에 맞는 요구사항 추정

캐시 노드의 크기를 결정할 때 서비스 요구사항을 고려하여  
레디스 서버가 어느 정도로 데이터를 처리해야 하는지 고려해야함
1. 전체 데이터양
2. 데이터가 최대치일 때 키의 개수
3. 하나의 키 당 평균 크기 및 최대 크기
4. 초당 키의 작업 수
5. 네트워크 트래픽
6. 실행되는 명령어 내역

이 중 가장 중요한 것은 `전체 데이터양`인데,  
레디스 클러스터가 아니라 레디스를 서버로 사용하는 경우 전체 데이터 양을 저장할 수 있는 메모리가 포함된 캐시 노드를 사용해야함  

또한 레디스 클러스터에서 데이터가 샤드 간에 균등하게 분산되는 경우는  
세심한 설계를 한 경우를 제외하고는 거의 없으므로 메모리 용량의 여유가 있는 노드를 권장  

또한 키 하나 당 크기가 크다면,  
예를 들어 Hash형의 키 하나에 매우 큰 해시가 저장되어 있으면  
샤드가 여러 개 있어도 데이터 분산이 어려울 수 있음  

#### 클라우드의 크기 조정

레디스를 클라우드 서비스를 통해 운영한다면,  
백업 기능 지원 여부, 노드 크기별 특성, 레디스 버전별 특징 등을 고려해야함  

기능과 제한 사항을 미리 확인하고, 레디스 클러스터를 사용할 때 비즈니스 요구사항을  
충족하는지 사전에 문서를 통해 확인해야함  

## 5.5 설정 파일 redis.conf
